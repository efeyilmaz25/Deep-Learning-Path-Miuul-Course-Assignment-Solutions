{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1: It Is Desired To Observe The Iteration, Gradient And Loss Changes For The Initial Weight In The Data Set.**"
      ],
      "metadata": {
        "id": "POYSALZWmXh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is expected that appropriate codes will be written to obtain the results in the output below."
      ],
      "metadata": {
        "id": "cRZ6h1EJnj-c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.250376  0.527516\n",
        "2           3            0.196769  0.441138\n",
        "3           4            0.163503  0.387700\n",
        "4           5            0.140597  0.350752\n",
        "..        ...                 ...       ...\n",
        "95         96            0.010033  0.108377\n",
        "96         97            0.009923  0.108025\n",
        "97         98            0.009816  0.107678\n",
        "98         99            0.009711  0.107337\n",
        "99        100            0.009608  0.107001\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```"
      ],
      "metadata": {
        "id": "8C1IxQUfnyTD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tips:**\n",
        "\n",
        "* The modeling process and all other operations are performed on all variables as in the contents.\n",
        "\n",
        "* Since we want the gradient for a single variable, it is sufficient to make a selection from the dw where the gradient calculation is made as follows; **\"df[0]\"**. Thus, the gradient information of the first variable is kept.\n",
        "\n",
        "* It may be useful to define a dictionary as follows before starting the loop inside the **\"gradient_descent\"** function;\n",
        "\n",
        "* grad_tracking = {'Iteration': [], 'Feature 1 Gradient': [], 'Loss': []}\n",
        "\n",
        "Afterwards, while the iterations are continuing, that is, while inside the for loop, it is sufficient to add the iteration information **(epoch +1)**, the gradient information of the first variable **(dw[0])** and the loss information **(loss)** in the relevant iteration to the grad_tracking dictionary.\n",
        "\n",
        "So, while we are inside the loop, we will be adding elements to a simple dictionary.\n",
        "\n",
        "* We don't need the predict function, our goal is to observe the gradients, iterations and loss.\n",
        "\n",
        "* The output of the gradient_descent function should simply return grad_tracking.\n",
        "\n",
        "* When calling the gradient_descent function, the arguments can be: gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "\n",
        "* The output will be ugly, so it would be a good idea to keep the output and then convert it to a dataframe."
      ],
      "metadata": {
        "id": "gGLqW_LZn8vN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1 Solution**"
      ],
      "metadata": {
        "id": "7IIJorismgC8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "def sigmoid(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def compute_loss(y, y_pred):\n",
        "    epsilon = 1e-5\n",
        "    return -np.mean(y * np.log(y_pred + epsilon) + (1 - y) * np.log(1 - y_pred + epsilon))\n",
        "\n",
        "def compute_gradients(X, y, y_pred):\n",
        "    return np.dot(X.T, (y_pred - y)) / len(y)\n",
        "\n",
        "def gradient_descent(X, y, lr=0.01, epochs=100):\n",
        "    weights = np.zeros(X.shape[1])\n",
        "    bias = 0\n",
        "\n",
        "    grad_tracking = {'Iteration': [], 'Feature 1 Gradient': [], 'Loss': []} # We define the dictionary we need.\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        z = np.dot(X, weights) + bias\n",
        "        y_pred = sigmoid(z)\n",
        "        loss = compute_loss(y, y_pred)\n",
        "        dw = compute_gradients(X, y, y_pred)\n",
        "        db = np.mean(y_pred - y)\n",
        "\n",
        "        grad_tracking['Iteration'].append(epoch+1) # We get iteration information\n",
        "        grad_tracking['Feature 1 Gradient'].append(dw[0]) # We get the gradient information.\n",
        "        grad_tracking['Loss'].append(loss) # We get loss information\n",
        "\n",
        "\n",
        "        weights -= lr * dw\n",
        "        bias -= lr * db\n",
        "\n",
        "    return grad_tracking"
      ],
      "metadata": {
        "id": "9sUchxQXqlZW"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad_tracking = gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "grad_tracking_df = pd.DataFrame(grad_tracking)\n",
        "print(grad_tracking_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qKm_zGZH5B27",
        "outputId": "8534e4b9-1a92-4379-b8a6-6233a578742c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.250376  0.527516\n",
            "2           3            0.196769  0.441138\n",
            "3           4            0.163503  0.387700\n",
            "4           5            0.140597  0.350752\n",
            "..        ...                 ...       ...\n",
            "95         96            0.010033  0.108377\n",
            "96         97            0.009923  0.108025\n",
            "97         98            0.009816  0.107678\n",
            "98         99            0.009711  0.107337\n",
            "99        100            0.009608  0.107001\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2: Save The Output Of The \"gradient_descent\" Function And Convert It To A Dataframe.**"
      ],
      "metadata": {
        "id": "H-qWCxp7mXbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Call the gradient descent function in the following 3 combinations and convert the results to dataframe and print them.\n",
        "\n",
        "1. gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "2. gradient_descent(X_train_scaled, y_train, lr=0.01, epochs=100)\n",
        "3. gradient_descent(X_train_scaled, y_train, lr=0.001, epochs=100)\n",
        "\n",
        "**Expected Output:**\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.250376  0.527516\n",
        "2           3            0.196769  0.441138\n",
        "3           4            0.163503  0.387700\n",
        "4           5            0.140597  0.350752\n",
        "..        ...                 ...       ...\n",
        "95         96            0.010033  0.108377\n",
        "96         97            0.009923  0.108025\n",
        "97         98            0.009816  0.107678\n",
        "98         99            0.009711  0.107337\n",
        "99        100            0.009608  0.107001\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.336617  0.673916\n",
        "2           3            0.326640  0.655887\n",
        "3           4            0.317060  0.638961\n",
        "4           5            0.307888  0.623058\n",
        "..        ...                 ...       ...\n",
        "95         96            0.085450  0.259625\n",
        "96         97            0.084800  0.258520\n",
        "97         98            0.084160  0.257431\n",
        "98         99            0.083530  0.256357\n",
        "99        100            0.082909  0.255299\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.345928  0.691178\n",
        "2           3            0.344896  0.689242\n",
        "3           4            0.343867  0.687318\n",
        "4           5            0.342841  0.685405\n",
        "..        ...                 ...       ...\n",
        "95         96            0.265483  0.552141\n",
        "96         97            0.264805  0.551030\n",
        "97         98            0.264130  0.549925\n",
        "98         99            0.263458  0.548826\n",
        "99        100            0.262789  0.547733\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```"
      ],
      "metadata": {
        "id": "6P_p1dRRvKT1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2 Solution**"
      ],
      "metadata": {
        "id": "Zsutdtlkmixj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "first_iteration_results = gradient_descent(X_train_scaled, y_train, lr=0.1, epochs=100)\n",
        "second_iteration_results = gradient_descent(X_train_scaled, y_train, lr=0.01, epochs=100)\n",
        "third_iteration_results = gradient_descent(X_train_scaled, y_train, lr=0.001, epochs=100)\n",
        "\n",
        "first_iteration_df = pd.DataFrame(first_iteration_results)\n",
        "second_iteration_df = pd.DataFrame(second_iteration_results)\n",
        "third_iteration_df = pd.DataFrame(third_iteration_results)"
      ],
      "metadata": {
        "id": "ycl7CssDvdBH"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(first_iteration_df)\n",
        "print(\"#\"*80)\n",
        "print(second_iteration_df)\n",
        "print(\"#\"*80)\n",
        "print(third_iteration_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2G3r4a6vwzW",
        "outputId": "3c64272d-39f0-4686-9ffb-b818e441198f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.250376  0.527516\n",
            "2           3            0.196769  0.441138\n",
            "3           4            0.163503  0.387700\n",
            "4           5            0.140597  0.350752\n",
            "..        ...                 ...       ...\n",
            "95         96            0.010033  0.108377\n",
            "96         97            0.009923  0.108025\n",
            "97         98            0.009816  0.107678\n",
            "98         99            0.009711  0.107337\n",
            "99        100            0.009608  0.107001\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "################################################################################\n",
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.336617  0.673916\n",
            "2           3            0.326640  0.655887\n",
            "3           4            0.317060  0.638961\n",
            "4           5            0.307888  0.623058\n",
            "..        ...                 ...       ...\n",
            "95         96            0.085450  0.259625\n",
            "96         97            0.084800  0.258520\n",
            "97         98            0.084160  0.257431\n",
            "98         99            0.083530  0.256357\n",
            "99        100            0.082909  0.255299\n",
            "\n",
            "[100 rows x 3 columns]\n",
            "################################################################################\n",
            "    Iteration  Feature 1 Gradient      Loss\n",
            "0           1            0.346963  0.693127\n",
            "1           2            0.345928  0.691178\n",
            "2           3            0.344896  0.689242\n",
            "3           4            0.343867  0.687318\n",
            "4           5            0.342841  0.685405\n",
            "..        ...                 ...       ...\n",
            "95         96            0.265483  0.552141\n",
            "96         97            0.264805  0.551030\n",
            "97         98            0.264130  0.549925\n",
            "98         99            0.263458  0.548826\n",
            "99        100            0.262789  0.547733\n",
            "\n",
            "[100 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3: How Do Loss Values Change As The Learning Rate Value Changes? What Is The Reason?**"
      ],
      "metadata": {
        "id": "mDYVxrcPmXZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the previous question, we used different learning rate values. How do the resulting loss values ​​change as a result of these different learning rate values? What is the reason? Explain."
      ],
      "metadata": {
        "id": "-biGVUfNwk66"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3 Solution**"
      ],
      "metadata": {
        "id": "pfOFasIcmkpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The learning rate value** **(lr)** in the **first iteration** is **0.1**. Here, the initial loss value is 0.693, and the loss value in the 100th iteration is 0.107. Since the learning rate value is relatively high here, the loss value decreases rapidly in the first few iterations. **With the learning rate value being 0.1, the model optimizes more aggressively and quickly.**\n",
        "\n",
        "**The learning rate value** in the **second iteration** is **0.01**. Here, the initial loss value is 0.693, and the loss value in the 100th iteration is 0.255. Here, the gradients decrease in smaller steps. **With the learning rate value being 0.01, the learning process is more controlled and occurs in smaller steps.**\n",
        "\n",
        "**The learning rate value** in the **third iteration** is **0.001**. Here, the initial loss value is 0.693, and the loss value in the 100th iteration is 0.547. Here, the gradients decrease quite slowly. **A learning rate value of 0.001 slows down the learning speed of the model significantly.**\n",
        "\n",
        "This is because in the gradient descent algorithm, the learning rate controls how much the model parameters are updated at each iteration. A large learning rate multiplies the gradient by a larger step, which causes the model parameters to change faster. A small learning rate multiplies the gradient by a smaller step, which causes the model parameters to change slower."
      ],
      "metadata": {
        "id": "vHEGmjOxwqfY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4: What Is The New Weight Value As A Result Of The First Update Rule?**"
      ],
      "metadata": {
        "id": "9XuetsOrmXW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "    Iteration  Feature 1 Gradient      Loss\n",
        "0           1            0.346963  0.693127\n",
        "1           2            0.336617  0.673916\n",
        "2           3            0.326640  0.655887\n",
        "3           4            0.317060  0.638961\n",
        "4           5            0.307888  0.623058\n",
        "..        ...                 ...       ...\n",
        "95         96            0.085450  0.259625\n",
        "96         97            0.084800  0.258520\n",
        "97         98            0.084160  0.257431\n",
        "98         99            0.083530  0.256357\n",
        "99        100            0.082909  0.255299\n",
        "\n",
        "[100 rows x 3 columns]\n",
        "```\n",
        "\n",
        "**We have these:**\n",
        "\n",
        "* learning rate: 0.001\n",
        "\n",
        "* Gradient value in the first iteration: 0.346963\n",
        "\n",
        "* Initial value of weight: 0\n",
        "\n",
        "In this case, what will be the new weight value as a result of the first update rule?"
      ],
      "metadata": {
        "id": "p_GRVRBb3mQo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4 Solution**"
      ],
      "metadata": {
        "id": "xwXIoWJFmmdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001\n",
        "gradient_value = 0.346963\n",
        "initial_weight = 0"
      ],
      "metadata": {
        "id": "F8GQ9-ry8nSC"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_weight = initial_weight - learning_rate * gradient_value"
      ],
      "metadata": {
        "id": "XJghdPV49W9c"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNxvwRjN9XXs",
        "outputId": "9fd7f31e-91f8-4a8e-8845-81112898df70"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.00034696300000000005"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}